{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4aad687-2d2f-467e-bfd6-fe13123c1c78",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 207866,
     "status": "ok",
     "timestamp": 1714555818137,
     "user": {
      "displayName": "Roberto Casaluce",
      "userId": "03502288058366201809"
     },
     "user_tz": -120
    },
    "id": "e4aad687-2d2f-467e-bfd6-fe13123c1c78",
    "outputId": "5da9d463-573a-48b7-f81c-5e582c4d019b"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from io import StringIO\n",
    "import time\n",
    "\n",
    "# This list will store DataFrames containing links\n",
    "dataframe_list = []\n",
    "\n",
    "# IMPORTANT: The number of pages on the website listing the companies is 1,045.\n",
    "# It's efficient to work in batches: range(0:100), range(100:200), range(200:230), etc., up to 1,046 (1,045 + 1).\n",
    "# You can use larger or smaller batches. Note that a 20-second delay between connections is set using time.sleep(20).\n",
    "\n",
    "for i in range(1046): \n",
    "    if i == 0:\n",
    "        url = 'https://www.fatturatoitalia.it/regione/friuli-venezia-giulia'\n",
    "    else:\n",
    "        url = f'https://www.fatturatoitalia.it/regione/friuli-venezia-giulia/{i + 1}'  # Add 1 to i to get the URL of the next page\n",
    "\n",
    "    print(url)\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # Raise an exception if the request was unsuccessful\n",
    "\n",
    "        html_table = BeautifulSoup(response.text, 'html.parser').find('table')\n",
    "        response.close()\n",
    "\n",
    "        # Find all rows in the table\n",
    "        rows = html_table.find_all('tr')\n",
    "\n",
    "        # Initialize an empty list for links in the first column\n",
    "        first_column_links = []\n",
    "\n",
    "        # Iterate over the table rows\n",
    "        for row in rows:\n",
    "            # Find the first cell in the row\n",
    "            cell = row.find('td')\n",
    "            # If the cell exists, search for a link inside it\n",
    "            if cell:\n",
    "                link = cell.find('a')\n",
    "                # If a link is found, add its href to the list of first column links\n",
    "                if link:\n",
    "                    first_column_links.append(link.get('href'))\n",
    "\n",
    "        # Convert the HTML string to a StringIO object\n",
    "        html_string = str(html_table)\n",
    "        html_io = StringIO(html_string)\n",
    "\n",
    "        # Read the DataFrame from the HTML string\n",
    "        df = pd.read_html(html_io, header=0)[0]\n",
    "\n",
    "        # Assign the links to the DataFrame\n",
    "        df['Link'] = first_column_links\n",
    "\n",
    "        dataframe_list.append(df)\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error during request to {url}: {e}\")\n",
    "\n",
    "    time.sleep(20)  # Wait for 20 seconds before the next request\n",
    "\n",
    "# Concatenate all DataFrames into a single DataFrame\n",
    "concatenated_df = pd.concat(dataframe_list, ignore_index=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "g9fs5wieZebL",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "executionInfo": {
     "elapsed": 302,
     "status": "ok",
     "timestamp": 1714555488859,
     "user": {
      "displayName": "Roberto Casaluce",
      "userId": "03502288058366201809"
     },
     "user_tz": -120
    },
    "id": "g9fs5wieZebL",
    "outputId": "e53bb2e8-7cf6-46ba-e5d9-187ac804d91b"
   },
   "outputs": [],
   "source": [
    "df_concatenato"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "YjbEYhk0Y7TY",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 333
    },
    "executionInfo": {
     "elapsed": 12919,
     "status": "error",
     "timestamp": 1714556068821,
     "user": {
      "displayName": "Roberto Casaluce",
      "userId": "03502288058366201809"
     },
     "user_tz": -120
    },
    "id": "YjbEYhk0Y7TY",
    "outputId": "a20b7178-e35b-46fb-ac9a-60551035e54c"
   },
   "outputs": [],
   "source": [
    "# This list will store DataFrames containing company information\n",
    "company_info_list = []\n",
    "\n",
    "for url in concatenated_df.Link:\n",
    "    # Fetch the webpage content\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        print(url)\n",
    "        # Check if the request was successful\n",
    "        if response.status_code == 200:\n",
    "            # Parse the HTML content\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "            # Extract specific information\n",
    "            # For example, extract the company name\n",
    "            company_name = soup.find('h1').get_text().strip()\n",
    "\n",
    "            # Extract the address\n",
    "            address = soup.find_all('div', class_='col-xs-7')[1].get_text().strip()\n",
    "\n",
    "            # Extract the city\n",
    "            city = soup.find_all('div', class_='col-xs-7')[2].get_text().strip()\n",
    "\n",
    "            # Extract the province\n",
    "            province = soup.find_all('div', class_='col-xs-7')[3].get_text().strip()\n",
    "\n",
    "            # Extract the VAT number\n",
    "            vat_number = soup.find_all('div', class_='col-xs-7')[5].get_text().strip()\n",
    "\n",
    "            # IMPORTANT: The numbers like [5] represent the 5th row of the table on each page.\n",
    "            # If you want additional information, you need to save the other rows and add them to the data dictionary.\n",
    "\n",
    "            # Create a DataFrame with the extracted data\n",
    "            data = {\n",
    "                'Company Name': [company_name],\n",
    "                'Address': [address],\n",
    "                'City': [city],\n",
    "                'Province': [province],\n",
    "                'VAT Number': [vat_number]\n",
    "            }\n",
    "\n",
    "            df = pd.DataFrame(data)\n",
    "            print(df)\n",
    "\n",
    "            company_info_list.append(df)\n",
    "\n",
    "        else:\n",
    "            print(\"Unable to retrieve the webpage.\")\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching {url}: {e}\")\n",
    "\n",
    "    time.sleep(20)  # Wait for 20 seconds before the next request\n",
    "\n",
    "# Concatenate all company information DataFrames into a single DataFrame\n",
    "final_company_info_df = pd.concat(company_info_list, ignore_index=True)\n",
    "\n",
    "# IMPORTANT: To save the file as CSV:\n",
    "# If working in batches, remember to change the filename to avoid overwriting each time\n",
    "final_company_info_df.to_csv('companies_batch_0_100.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a5b39a-cbf9-4712-8aeb-a10af0395dc9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
